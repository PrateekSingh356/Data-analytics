{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7f0be6a",
   "metadata": {},
   "source": [
    "# Python Advance Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459deebe",
   "metadata": {},
   "source": [
    "# Q1. What is the process for loading a dataset from an external source?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "909da115",
   "metadata": {},
   "source": [
    "Manual function\n",
    "loadtxt function\n",
    "genfromtxt function\n",
    "read_csv function\n",
    "Pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b43c547",
   "metadata": {},
   "source": [
    "# Q2. How can we use pandas to read JSON files?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae9a03",
   "metadata": {},
   "source": [
    "To read the files, we use read_json() function and through it, we pass the path to the JSON file we want to read. Once we do that, it returns a “DataFrame”( A table of rows and columns) that stores data. If we want to read a file that is located on remote servers then we pass the link to its location instead of a local path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cae67d8",
   "metadata": {},
   "source": [
    "# Q3. Describe the significance of DASK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bced4d",
   "metadata": {},
   "source": [
    "Dask is a open-source library that provides advanced parallelization for analytics, especially when you are working with large data.It is built to help you improve code performance and scale-up without having to re-write your entire code. The good thing is, you can use all your favorite python libraries as Dask is built in coordination with numpy, scikit-learn, scikit-image, pandas, xgboost, RAPIDS and others.\n",
    "\n",
    "That means you can now use Dask to not only speed up computations on datasets using parallel processing, but also build ML models using scikit-learn, XGBoost on much larger datasets.\n",
    "\n",
    "You can use it to scale your python code for data analysis. If you think, this sounds a bit complicated to implement, just read on.\n",
    "as your data gets bigger, bigger than what you can fit in the RAM, pandas won’t be sufficient.\n",
    "This is a very common problem.\n",
    "\n",
    "You may use Spark or Hadoop to solve this. But, these are not python environments. This stops you from using numpy, sklearn, pandas, tensorflow, and all the commonly used Python libraries for ML.\n",
    "\n",
    "Is there a solution for this?\n",
    "\n",
    "Yes! This is where Dask comes in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c18118",
   "metadata": {},
   "source": [
    "# Q4. Describe the functions of DASK."
   ]
  },
  {
   "cell_type": "raw",
   "id": "55b053e6",
   "metadata": {},
   "source": [
    "-Dask DataFrame mimics Pandas:\n",
    "\n",
    "import pandas as pd                     import dask.dataframe as dd\n",
    "df = pd.read_csv('2015-01-01.csv')      df = dd.read_csv('2015-*-*.csv')\n",
    "df.groupby(df.user_id).value.mean()     df.groupby(df.user_id).value.mean().compute()\n",
    "\n",
    "-Dask Array mimics NumPy:\n",
    "\n",
    "import numpy as np                       import dask.array as da\n",
    "f = h5py.File('myfile.hdf5')             f = h5py.File('myfile.hdf5')\n",
    "x = np.array(f['/small-data'])           x = da.from_array(f['/big-data'],\n",
    "                                                           chunks=(1000, 1000))\n",
    "x - x.mean(axis=1)                       x - x.mean(axis=1).compute()\n",
    "\n",
    "-Dask Bag mimics iterators, Toolz, and PySpark:\n",
    "\n",
    "import dask.bag as db\n",
    "b = db.read_text('2015-*-*.json.gz').map(json.loads)\n",
    "b.pluck('name').frequencies().topk(10, lambda pair: pair[1]).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1cb3af",
   "metadata": {},
   "source": [
    "# Q5. Describe Cassandra's features."
   ]
  },
  {
   "cell_type": "raw",
   "id": "73c9cc84",
   "metadata": {},
   "source": [
    "Hybrid:\n",
    "Cassandra architecture is very robust.\n",
    "At the same time it is very flexible.\n",
    "It has very low latency as data is stored over many nodes.\n",
    "An entire data centre of any means can handle with no single data loss.\n",
    "It can perform across public and private clouds with no major difference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac992dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
